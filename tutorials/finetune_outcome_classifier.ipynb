{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning on Pre-trained Model for Patient Outcome Classification\n",
    "Fine-tune scGPT model on merged dataset for the patient outcome classification to see if model performance improves with increased patient number. We use the CD19 (axicel) and CD19/CD22 CAR clinical trial dataset to fine-tune on the pre-trained whole-body model. \n",
    "\n",
    "We summarize the fine-tuning pipeline in the following steps, which can be used as a general recipe for finetuning on cell-type annotation tasks and beyond: \n",
    "\n",
    "     1. Specify hyper-parameter setup for integration task\n",
    "     \n",
    "     2. Load and pre-process data\n",
    "     \n",
    "     3. Load the pre-trained scGPT model\n",
    "     \n",
    "     4. Finetune scGPT with task-specific objectives\n",
    "     \n",
    "     5. Evaluate fine-tuned scGPT\n",
    "workflow: finetune model using CART data, label: T cell subset -> save model -> finetune on perturb --> predict few genes --> visualization \n",
    "possible label:\n",
    "- patient outcome\n",
    "- T cell subset annotation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create more bins to see how model performance improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/home/bench-user/.apps/conda/envs/scGPT_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/bench-user/scGPT/Tcell_GPT/tutorials/../scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/bench-user/scGPT/Tcell_GPT/tutorials/../scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/bench-user/.apps/conda/envs/scGPT_env/lib/python3.10/site-packages/scanpy/_settings.py:488: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  IPython.display.set_matplotlib_formats(*ipython_format)\n",
      "/home/bench-user/.apps/conda/envs/scGPT_env/lib/python3.10/site-packages/IPython/core/pylabtools.py:77: DeprecationWarning: backend2gui is deprecated since IPython 8.24, backends are managed in matplotlib and can be externally registered.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import pickle\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class TransformerModel(nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        \n",
      "        ntoken: int,\n",
      "        d_model: int,\n",
      "        nhead: int,\n",
      "        d_hid: int,\n",
      "        nlayers: int,\n",
      "        nlayers_cls: int = 3,\n",
      "        n_cls: int = 1,\n",
      "        vocab: Any = None,\n",
      "        dropout: float = 0.5,\n",
      "        pad_token: str = \"<pad>\",\n",
      "        pad_value: int = 0,\n",
      "        do_mvc: bool = False,\n",
      "        do_dab: bool = False,\n",
      "        use_batch_labels: bool = False,\n",
      "        num_batch_labels: Optional[int] = None,\n",
      "        domain_spec_batchnorm: Union[bool, str] = False,\n",
      "        input_emb_style: str = \"continuous\",\n",
      "        n_input_bins: Optional[int] = None,\n",
      "        cell_emb_style: str = \"cls\",\n",
      "        mvc_decoder_style: str = \"inner product\",\n",
      "        ecs_threshold: float = 0.3,\n",
      "        explicit_zero_prob: bool = False,\n",
      "        use_fast_transformer: bool = False,\n",
      "        fast_transformer_backend: str = \"flash\",\n",
      "        pre_norm: bool = False,\n",
      "        transfer_learning: bool = False,\n",
      "        transfer_hidden_dim: int = 256,\n",
      "        transfer_dropout: float = 0.1,\n",
      "        \n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.model_type = \"Transformer\"\n",
      "        self.d_model = d_model\n",
      "        self.do_dab = do_dab\n",
      "        self.ecs_threshold = ecs_threshold\n",
      "        self.use_batch_labels = use_batch_labels\n",
      "        self.domain_spec_batchnorm = domain_spec_batchnorm\n",
      "        self.input_emb_style = input_emb_style\n",
      "        self.cell_emb_style = cell_emb_style\n",
      "        self.explicit_zero_prob = explicit_zero_prob\n",
      "        self.norm_scheme = \"pre\" if pre_norm else \"post\"\n",
      "        \n",
      "        # two layer binary classifier for transfer learning\n",
      "        self.transfer_learning = transfer_learning\n",
      "        if transfer_learning:\n",
      "            self.transfer_classifier = nn.Sequential(\n",
      "                nn.Linear(d_model, transfer_hidden_dim),\n",
      "                nn.ReLU(),\n",
      "                nn.Dropout(transfer_dropout),\n",
      "                nn.Linear(transfer_hidden_dim, transfer_hidden_dim //2),\n",
      "                nn.ReLU(),\n",
      "                nn.Dropout(transfer_dropout),\n",
      "                nn.Linear(transfer_hidden_dim //2 , transfer_hidden_dim //4),\n",
      "                nn.ReLU(),\n",
      "                nn.Dropout(transfer_dropout),\n",
      "                nn.Linear(transfer_hidden_dim //4, n_cls)\n",
      "            )\n",
      "            \n",
      "        if self.input_emb_style not in [\"category\", \"continuous\", \"scaling\"]:\n",
      "            raise ValueError(\n",
      "                f\"input_emb_style should be one of category, continuous, scaling, \"\n",
      "                f\"got {input_emb_style}\"\n",
      "            )\n",
      "        if cell_emb_style not in [\"cls\", \"avg-pool\", \"w-pool\"]:\n",
      "            raise ValueError(f\"Unknown cell_emb_style: {cell_emb_style}\")\n",
      "        if use_fast_transformer:\n",
      "            if not flash_attn_available:\n",
      "                warnings.warn(\n",
      "                    \"flash-attn is not installed, using pytorch transformer instead. \"\n",
      "                    \"Set use_fast_transformer=False to avoid this warning. \"\n",
      "                    \"Installing flash-attn is highly recommended.\"\n",
      "                )\n",
      "                use_fast_transformer = False\n",
      "        self.use_fast_transformer = use_fast_transformer\n",
      "\n",
      "        # TODO: add dropout in the GeneEncoder\n",
      "        self.encoder = GeneEncoder(ntoken, d_model, padding_idx=vocab[pad_token])\n",
      "\n",
      "        # Value Encoder, NOTE: the scaling style is also handled in _encode method\n",
      "        if input_emb_style == \"continuous\":\n",
      "            self.value_encoder = ContinuousValueEncoder(d_model, dropout)\n",
      "        elif input_emb_style == \"category\":\n",
      "            assert n_input_bins > 0\n",
      "            self.value_encoder = CategoryValueEncoder(\n",
      "                n_input_bins, d_model, padding_idx=pad_value\n",
      "            )\n",
      "        else:\n",
      "            self.value_encoder = nn.Identity()  # nn.Softmax(dim=1)\n",
      "            # TODO: consider row-wise normalization or softmax\n",
      "            # TODO: Correct handle the mask_value when using scaling\n",
      "\n",
      "        # Batch Encoder\n",
      "        if use_batch_labels:\n",
      "            self.batch_encoder = BatchLabelEncoder(num_batch_labels, d_model)\n",
      "\n",
      "        if domain_spec_batchnorm is True or domain_spec_batchnorm == \"dsbn\":\n",
      "            use_affine = True if domain_spec_batchnorm == \"do_affine\" else False\n",
      "            print(f\"Use domain specific batchnorm with affine={use_affine}\")\n",
      "            self.dsbn = DomainSpecificBatchNorm1d(\n",
      "                d_model, num_batch_labels, eps=6.1e-5, affine=use_affine\n",
      "            )\n",
      "        elif domain_spec_batchnorm == \"batchnorm\":\n",
      "            print(\"Using simple batchnorm instead of domain specific batchnorm\")\n",
      "            self.bn = nn.BatchNorm1d(d_model, eps=6.1e-5)\n",
      "\n",
      "        if use_fast_transformer:\n",
      "            if fast_transformer_backend == \"linear\":\n",
      "                self.transformer_encoder = FastTransformerEncoderWrapper(\n",
      "                    d_model, nhead, d_hid, nlayers, dropout\n",
      "                )\n",
      "            elif fast_transformer_backend == \"flash\":\n",
      "                encoder_layers = FlashTransformerEncoderLayer(\n",
      "                    d_model,\n",
      "                    nhead,\n",
      "                    d_hid,\n",
      "                    dropout,\n",
      "                    batch_first=True,\n",
      "                    norm_scheme=self.norm_scheme,\n",
      "                )\n",
      "                self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
      "        else:\n",
      "            encoder_layers = TransformerEncoderLayer(\n",
      "                d_model, nhead, d_hid, dropout, batch_first=True\n",
      "            )\n",
      "            self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
      "\n",
      "        self.decoder = ExprDecoder(\n",
      "            d_model,\n",
      "            explicit_zero_prob=explicit_zero_prob,\n",
      "            use_batch_labels=use_batch_labels,\n",
      "        )\n",
      "        self.cls_decoder = ClsDecoder(d_model, n_cls, nlayers=nlayers_cls)\n",
      "        if do_mvc:\n",
      "            self.mvc_decoder = MVCDecoder(\n",
      "                d_model,\n",
      "                arch_style=mvc_decoder_style,\n",
      "                explicit_zero_prob=explicit_zero_prob,\n",
      "                use_batch_labels=use_batch_labels,\n",
      "            )\n",
      "\n",
      "        if do_dab:\n",
      "            self.grad_reverse_discriminator = AdversarialDiscriminator(\n",
      "                d_model,\n",
      "                n_cls=num_batch_labels,\n",
      "                reverse_grad=True,\n",
      "            )\n",
      "\n",
      "        self.sim = Similarity(temp=0.5)  # TODO: auto set temp\n",
      "        self.creterion_cce = nn.CrossEntropyLoss()\n",
      "\n",
      "        self.init_weights()\n",
      "\n",
      "    def init_weights(self) -> None:\n",
      "        initrange = 0.1\n",
      "        # TODO: check if this initialization is helpful and shall we apply to all?\n",
      "        self.encoder.embedding.weight.data.uniform_(-initrange, initrange)\n",
      "\n",
      "    def _encode(\n",
      "        self,\n",
      "        src: Tensor,\n",
      "        values: Tensor,\n",
      "        src_key_padding_mask: Tensor,\n",
      "        batch_labels: Optional[Tensor] = None,  # (batch,)\n",
      "    ) -> Tensor:\n",
      "        self._check_batch_labels(batch_labels)\n",
      "\n",
      "        src = self.encoder(src)  # (batch, seq_len, embsize)\n",
      "        self.cur_gene_token_embs = src\n",
      "\n",
      "        values = self.value_encoder(values)  # (batch, seq_len, embsize)\n",
      "        if self.input_emb_style == \"scaling\":\n",
      "            values = values.unsqueeze(2)\n",
      "            total_embs = src * values\n",
      "        else:\n",
      "            total_embs = src + values\n",
      "\n",
      "        if getattr(self, \"dsbn\", None) is not None:\n",
      "            batch_label = int(batch_labels[0].item())\n",
      "            total_embs = self.dsbn(total_embs.permute(0, 2, 1), batch_label).permute(\n",
      "                0, 2, 1\n",
      "            )  # the batch norm always works on dim 1\n",
      "        elif getattr(self, \"bn\", None) is not None:\n",
      "            total_embs = self.bn(total_embs.permute(0, 2, 1)).permute(0, 2, 1)\n",
      "\n",
      "        output = self.transformer_encoder(\n",
      "            total_embs, src_key_padding_mask=src_key_padding_mask\n",
      "        )\n",
      "        return output  # (batch, seq_len, embsize)\n",
      "    \n",
      "    # following 2 functions are added to load the pretrained\n",
      "    \n",
      "    # def load_pretrained_weights(self, pretrained_state_dict, freeze_encoder=True):\n",
      "    #     '''\n",
      "    #     Load pretrained weights and optionally freeze the encoder.\n",
      "\n",
      "    #     Args:\n",
      "    #         pretrained_state_dict (dict): The state dict of the pretrained model.\n",
      "    #         freeze_encoder (bool): if True, freeze the encoder.\n",
      "    #     '''\n",
      "    #     model_state_dict = self.state_dict()\n",
      "    #     pretrained_state_dict = {k:v for k,v in pretrained_state_dict.items() if k in model_state_dict}\n",
      "\n",
      "    #     # Load the filtered pretrained weights\n",
      "    #     self.load_state_dict(pretrained_state_dict, strict=False)\n",
      "\n",
      "    #     if freeze_encoder:\n",
      "    #         for name, param in self.named_parameters():\n",
      "    #             if 'encoder' in name or 'transformer_encoder' in name:\n",
      "    #                 param.required_grad = False\n",
      "\n",
      "    # def unfreeze_encoder(self):\n",
      "    #     '''\n",
      "    #     Unfreeze the encoder for finetuning'''\n",
      "    #     for name, param in self.named_parameters():\n",
      "    #         if 'encoder' in name or 'transformer_encoder' in name:\n",
      "    #             param.requires_grad = True\n",
      "\n",
      "\n",
      "    def _get_cell_emb_from_layer(\n",
      "        self, layer_output: Tensor, weights: Tensor = None\n",
      "    ) -> Tensor:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            layer_output(:obj:`Tensor`): shape (batch, seq_len, embsize)\n",
      "            weights(:obj:`Tensor`): shape (batch, seq_len), optional and only used\n",
      "                when :attr:`self.cell_emb_style` is \"w-pool\".\n",
      "\n",
      "        Returns:\n",
      "            :obj:`Tensor`: shape (batch, embsize)\n",
      "        \"\"\"\n",
      "        if self.cell_emb_style == \"cls\":\n",
      "            cell_emb = layer_output[:, 0, :]  # (batch, embsize)\n",
      "        elif self.cell_emb_style == \"avg-pool\":\n",
      "            cell_emb = torch.mean(layer_output, dim=1)\n",
      "        elif self.cell_emb_style == \"w-pool\":\n",
      "            if weights is None:\n",
      "                raise ValueError(\"weights is required when cell_emb_style is w-pool\")\n",
      "            if weights.dim() != 2:\n",
      "                raise ValueError(\"weights should be 2D\")\n",
      "            cell_emb = torch.sum(layer_output * weights.unsqueeze(2), dim=1)\n",
      "            cell_emb = F.normalize(cell_emb, p=2, dim=1)  # (batch, embsize)\n",
      "\n",
      "        return cell_emb\n",
      "\n",
      "    def _check_batch_labels(self, batch_labels: Tensor) -> None:\n",
      "        if self.use_batch_labels or self.domain_spec_batchnorm:\n",
      "            assert batch_labels is not None\n",
      "        elif batch_labels is not None:\n",
      "            raise ValueError(\n",
      "                \"batch_labels should only be provided when `self.use_batch_labels`\"\n",
      "                \" or `self.domain_spec_batchnorm` is True\"\n",
      "            )\n",
      "\n",
      "    def generate(\n",
      "        self,\n",
      "        cell_emb: Tensor,\n",
      "        src: Tensor,\n",
      "        values: Optional[Tensor] = None,\n",
      "        src_key_padding_mask: Optional[Tensor] = None,\n",
      "        gen_iters: int = 1,\n",
      "        batch_labels: Optional[Tensor] = None,  # (batch,)\n",
      "    ) -> Tensor:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            cell_emb(:obj:`Tensor`): shape (batch, embsize)\n",
      "            src(:obj:`Tensor`): shape (batch, seq_len)\n",
      "            values(:obj:`Tensor`): shape (batch, seq_len), optional\n",
      "            src_key_padding_mask(:obj:`Tensor`): shape (batch, seq_len), optional\n",
      "            gen_iters(:obj:`int`): number of generation iterations\n",
      "            batch_labels(:obj:`Tensor`): shape (batch,), optional\n",
      "        \"\"\"\n",
      "        # TODO: should have a tag indicate the generation mode\n",
      "        # TODO: if gen_iters > 1, should have a tag indicate the current iteration\n",
      "        try:\n",
      "            self._check_batch_labels(batch_labels)\n",
      "        except:\n",
      "            import warnings\n",
      "\n",
      "            warnings.warn(\n",
      "                \"batch_labels is required but not provided, using zeros instead\"\n",
      "            )\n",
      "            batch_labels = torch.zeros(\n",
      "                cell_emb.shape[0], dtype=torch.long, device=cell_emb.device\n",
      "            )\n",
      "\n",
      "        src = self.encoder(src)  # (batch, seq_len, embsize)\n",
      "\n",
      "        if values is not None:\n",
      "            values = self.value_encoder(values)  # (batch, seq_len, embsize)\n",
      "            if self.input_emb_style == \"scaling\":\n",
      "                values = values.unsqueeze(2)\n",
      "                total_embs = src * values\n",
      "            else:\n",
      "                total_embs = src + values\n",
      "        else:\n",
      "            total_embs = src\n",
      "\n",
      "        if getattr(self, \"dsbn\", None) is not None:\n",
      "            batch_label = int(batch_labels[0].item())\n",
      "            total_embs = self.dsbn(total_embs.permute(0, 2, 1), batch_label).permute(\n",
      "                0, 2, 1\n",
      "            )  # the batch norm always works on dim 1\n",
      "        elif getattr(self, \"bn\", None) is not None:\n",
      "            total_embs = self.bn(total_embs.permute(0, 2, 1)).permute(0, 2, 1)\n",
      "\n",
      "        total_embs[:, 0, :] = cell_emb\n",
      "\n",
      "        if src_key_padding_mask is None:\n",
      "            src_key_padding_mask = torch.zeros(\n",
      "                total_embs.shape[:2], dtype=torch.bool, device=total_embs.device\n",
      "            )\n",
      "        transformer_output = self.transformer_encoder(\n",
      "            total_embs, src_key_padding_mask=src_key_padding_mask\n",
      "        )\n",
      "\n",
      "        if self.use_batch_labels:\n",
      "            batch_emb = self.batch_encoder(batch_labels)  # (batch, embsize)\n",
      "        mlm_output = self.decoder(\n",
      "            transformer_output\n",
      "            if not self.use_batch_labels\n",
      "            else torch.cat(\n",
      "                [\n",
      "                    transformer_output,\n",
      "                    batch_emb.unsqueeze(1).repeat(1, transformer_output.shape[1], 1),\n",
      "                ],\n",
      "                dim=2,\n",
      "            ),\n",
      "            # else transformer_output + batch_emb.unsqueeze(1),\n",
      "        )\n",
      "        output = mlm_output[\"pred\"]  # (batch, seq_len)\n",
      "\n",
      "        return output  # (batch, seq_len)\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        src: Tensor,\n",
      "        values: Tensor,\n",
      "        src_key_padding_mask: Tensor,\n",
      "        batch_labels: Optional[Tensor] = None,\n",
      "        transfer_mode: bool = False,\n",
      "        CLS: bool = False,\n",
      "        CCE: bool = False,\n",
      "        MVC: bool = False,\n",
      "        ECS: bool = False,\n",
      "        do_sample: bool = False,\n",
      "    ) -> Mapping[str, Tensor]:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            src (:obj:`Tensor`): token ids, shape [batch_size, seq_len]\n",
      "            values (:obj:`Tensor`): token values, shape [batch_size, seq_len]\n",
      "            src_key_padding_mask (:obj:`Tensor`): mask for src, shape [batch_size,\n",
      "                seq_len]\n",
      "            batch_labels (:obj:`Tensor`): batch labels, shape [batch_size]\n",
      "            CLS (:obj:`bool`): if True, return the celltype classification objective\n",
      "                (CLS) output\n",
      "            CCE (:obj:`bool`): if True, return the contrastive cell embedding objective\n",
      "                (CCE) output\n",
      "            MVC (:obj:`bool`): if True, return the masked value prediction for cell\n",
      "                embedding MVC output\n",
      "            ECS (:obj:`bool`): if True, return the elastic cell similarity objective\n",
      "                (ECS) output.\n",
      "\n",
      "        Returns:\n",
      "            dict of output Tensors.\n",
      "        \"\"\"\n",
      "        transformer_output = self._encode(\n",
      "            src, values, src_key_padding_mask, batch_labels\n",
      "        )\n",
      "\n",
      "        cell_emb = self._get_cell_emb_from_layer(transformer_output, values)\n",
      "\n",
      "        output = {}\n",
      "        output[\"cell_emb\"] = cell_emb\n",
      "\n",
      "        if transfer_mode and self.transfer_learning:\n",
      "            # use the transfer learning classifier\n",
      "            transfer_output = self.transfer_classifier(cell_emb)\n",
      "            output[\"transfer_output\"] = transfer_output\n",
      "        else:\n",
      "            # Use the original pre-training task\n",
      "            if self.use_batch_labels:\n",
      "                batch_emb = self.batch_encoder(batch_labels)  # (batch, embsize)\n",
      "\n",
      "        # output = {} # if this in the original code\n",
      "            mlm_output = self.decoder(\n",
      "                transformer_output\n",
      "                if not self.use_batch_labels\n",
      "                else torch.cat(\n",
      "                    [\n",
      "                        transformer_output,\n",
      "                        batch_emb.unsqueeze(1).repeat(1, transformer_output.shape[1], 1),\n",
      "                    ],\n",
      "                    dim=2,\n",
      "                ),\n",
      "                # else transformer_output + batch_emb.unsqueeze(1),\n",
      "            )\n",
      "            if self.explicit_zero_prob and do_sample:\n",
      "                bernoulli = Bernoulli(probs=mlm_output[\"zero_probs\"])\n",
      "                output[\"mlm_output\"] = bernoulli.sample() * mlm_output[\"pred\"]\n",
      "            else:\n",
      "                output[\"mlm_output\"] = mlm_output[\"pred\"]  # (batch, seq_len)\n",
      "            if self.explicit_zero_prob:\n",
      "                output[\"mlm_zero_probs\"] = mlm_output[\"zero_probs\"]\n",
      "\n",
      "            cell_emb = self._get_cell_emb_from_layer(transformer_output, values)\n",
      "            output[\"cell_emb\"] = cell_emb\n",
      "\n",
      "            if CLS:\n",
      "                output[\"cls_output\"] = self.cls_decoder(cell_emb)  # (batch, n_cls)\n",
      "            if CCE:\n",
      "                cell1 = cell_emb\n",
      "                transformer_output2 = self._encode(\n",
      "                    src, values, src_key_padding_mask, batch_labels\n",
      "                )\n",
      "                cell2 = self._get_cell_emb_from_layer(transformer_output2)\n",
      "\n",
      "                # Gather embeddings from all devices if distributed training\n",
      "                if dist.is_initialized() and self.training:\n",
      "                    cls1_list = [\n",
      "                        torch.zeros_like(cell1) for _ in range(dist.get_world_size())\n",
      "                    ]\n",
      "                    cls2_list = [\n",
      "                        torch.zeros_like(cell2) for _ in range(dist.get_world_size())\n",
      "                    ]\n",
      "                    dist.all_gather(tensor_list=cls1_list, tensor=cell1.contiguous())\n",
      "                    dist.all_gather(tensor_list=cls2_list, tensor=cell2.contiguous())\n",
      "\n",
      "                    # NOTE: all_gather results have no gradients, so replace the item\n",
      "                    # of the current rank with the original tensor to keep gradients.\n",
      "                    # See https://github.com/princeton-nlp/SimCSE/blob/main/simcse/models.py#L186\n",
      "                    cls1_list[dist.get_rank()] = cell1\n",
      "                    cls2_list[dist.get_rank()] = cell2\n",
      "\n",
      "                    cell1 = torch.cat(cls1_list, dim=0)\n",
      "                    cell2 = torch.cat(cls2_list, dim=0)\n",
      "                # TODO: should detach the second run cls2? Can have a try\n",
      "                cos_sim = self.sim(cell1.unsqueeze(1), cell2.unsqueeze(0))  # (batch, batch)\n",
      "                labels = torch.arange(cos_sim.size(0)).long().to(cell1.device)\n",
      "                output[\"loss_cce\"] = self.creterion_cce(cos_sim, labels)\n",
      "            if MVC:\n",
      "                mvc_output = self.mvc_decoder(\n",
      "                    cell_emb\n",
      "                    if not self.use_batch_labels\n",
      "                    else torch.cat([cell_emb, batch_emb], dim=1),\n",
      "                    # else cell_emb + batch_emb,\n",
      "                    self.cur_gene_token_embs,\n",
      "                )\n",
      "                if self.explicit_zero_prob and do_sample:\n",
      "                    bernoulli = Bernoulli(probs=mvc_output[\"zero_probs\"])\n",
      "                    output[\"mvc_output\"] = bernoulli.sample() * mvc_output[\"pred\"]\n",
      "                else:\n",
      "                    output[\"mvc_output\"] = mvc_output[\"pred\"]  # (batch, seq_len)\n",
      "                if self.explicit_zero_prob:\n",
      "                    output[\"mvc_zero_probs\"] = mvc_output[\"zero_probs\"]\n",
      "            if ECS:\n",
      "                # Here using customized cosine similarity instead of F.cosine_similarity\n",
      "                # to avoid the pytorch issue of similarity larger than 1.0, pytorch # 78064\n",
      "                # normalize the embedding\n",
      "                cell_emb_normed = F.normalize(cell_emb, p=2, dim=1)\n",
      "                cos_sim = torch.mm(cell_emb_normed, cell_emb_normed.t())  # (batch, batch)\n",
      "\n",
      "                # mask out diagnal elements\n",
      "                mask = torch.eye(cos_sim.size(0)).bool().to(cos_sim.device)\n",
      "                cos_sim = cos_sim.masked_fill(mask, 0.0)\n",
      "                # only optimize positive similarities\n",
      "                cos_sim = F.relu(cos_sim)\n",
      "\n",
      "                output[\"loss_ecs\"] = torch.mean(1 - (cos_sim - self.ecs_threshold) ** 2)\n",
      "\n",
      "            if self.do_dab:\n",
      "                output[\"dab_output\"] = self.grad_reverse_discriminator(cell_emb)\n",
      "\n",
      "        return output\n",
      "\n",
      "    def encode_batch(\n",
      "        self,\n",
      "        src: Tensor,\n",
      "        values: Tensor,\n",
      "        src_key_padding_mask: Tensor,\n",
      "        batch_size: int,\n",
      "        batch_labels: Optional[Tensor] = None,\n",
      "        output_to_cpu: bool = True,\n",
      "        time_step: Optional[int] = None,\n",
      "        return_np: bool = False,\n",
      "    ) -> Tensor:\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            src (Tensor): shape [N, seq_len]\n",
      "            values (Tensor): shape [N, seq_len]\n",
      "            src_key_padding_mask (Tensor): shape [N, seq_len]\n",
      "            batch_size (int): batch size for encoding\n",
      "            batch_labels (Tensor): shape [N, n_batch_labels]\n",
      "            output_to_cpu (bool): whether to move the output to cpu\n",
      "            time_step (int): the time step index in the transformer output to return.\n",
      "                The time step is along the second dimenstion. If None, return all.\n",
      "            return_np (bool): whether to return numpy array\n",
      "\n",
      "        Returns:\n",
      "            output Tensor of shape [N, seq_len, embsize]\n",
      "        \"\"\"\n",
      "        N = src.size(0)\n",
      "        device = next(self.parameters()).device\n",
      "\n",
      "        # initialize the output tensor\n",
      "        array_func = np.zeros if return_np else torch.zeros\n",
      "        float32_ = np.float32 if return_np else torch.float32\n",
      "        shape = (\n",
      "            (N, self.d_model)\n",
      "            if time_step is not None\n",
      "            else (N, src.size(1), self.d_model)\n",
      "        )\n",
      "        outputs = array_func(shape, dtype=float32_)\n",
      "\n",
      "        for i in trange(0, N, batch_size):\n",
      "            raw_output = self._encode(\n",
      "                src[i : i + batch_size].to(device),\n",
      "                values[i : i + batch_size].to(device),\n",
      "                src_key_padding_mask[i : i + batch_size].to(device),\n",
      "                batch_labels[i : i + batch_size].to(device)\n",
      "                if batch_labels is not None\n",
      "                else None,\n",
      "            )\n",
      "            output = raw_output.detach()\n",
      "            if output_to_cpu:\n",
      "                output = output.cpu()\n",
      "            if return_np:\n",
      "                output = output.numpy()\n",
      "            if time_step is not None:\n",
      "                output = output[:, time_step, :]\n",
      "            outputs[i : i + batch_size] = output\n",
      "\n",
      "        return outputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import scgpt.model\n",
    "\n",
    "# Get the source code of the TransformerModel class\n",
    "source = inspect.getsource(scgpt.model.TransformerModel)\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with aggreagted dataset to see if performance improves\n",
    "\n",
    "Previous versions tested: Test with pp_B_prod_data_v2\n",
    "\n",
    "try one meta data at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Specify hyper-parameter setup for cell-type annotation task\n",
    "Listed below are some hyper-parameter recommendations for the cell-type task. Note that the CLS objective is on to facilitate cell-type classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    dataset_name=\"clonotype_genes_filtered_counts_data\",\n",
    "    do_train=True,\n",
    "    load_model=\"../model\",\n",
    "    mask_ratio=0.00, # from 0.00\n",
    "    epochs=3, #from 5\n",
    "    n_bins=101,\n",
    "    MVC=False, # False, Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0, # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0, # 0.0, # DAR objective weight for multi-omic batch correction\n",
    "    lr= 1e-3, #1e-4, # higher lr since we are finetuning\n",
    "    batch_size=8,\n",
    "    layer_size=128,\n",
    "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=4,  # number of heads in nn.Multihea dAttention\n",
    "    dropout=0.2,  # dropout probability\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    include_zero_gene = False,\n",
    "    freeze = False, #freeze\n",
    "    DSBN = False,  # Domain-spec batchnorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkristint\u001b[0m (\u001b[33mmackall_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bench-user/scGPT/Tcell_GPT/tutorials/wandb/run-20240719_185530-b6i80sb3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mackall_lab/finetune_tcellGPT/runs/b6i80sb3/workspace' target=\"_blank\">summer-plasma-7</a></strong> to <a href='https://wandb.ai/mackall_lab/finetune_tcellGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mackall_lab/finetune_tcellGPT' target=\"_blank\">https://wandb.ai/mackall_lab/finetune_tcellGPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mackall_lab/finetune_tcellGPT/runs/b6i80sb3/workspace' target=\"_blank\">https://wandb.ai/mackall_lab/finetune_tcellGPT/runs/b6i80sb3/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'dataset_name': 'clonotype_genes_filtered_counts_data', 'do_train': True, 'load_model': '../model', 'mask_ratio': 0.0, 'epochs': 3, 'n_bins': 101, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.001, 'batch_size': 8, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': True, 'pre_norm': False, 'amp': True, 'include_zero_gene': False, 'freeze': False, 'DSBN': False}\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"finetune_tcellGPT\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "include_zero_gene = config.include_zero_gene  # if True, include zero genes among hvgs in the training\n",
    "max_seq_len = 3001\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "output_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "\n",
    "# settings for training\n",
    "MLM = False  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = True  # response classification objective\n",
    "ADV = False  # Adversarial training for batch correction\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = config.MVC  # Masked value prediction for cell embedding\n",
    "ECS = config.ecs_thres > 0  # Elastic cell similarity objective\n",
    "DAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\n",
    "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
    "input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
    "adv_D_delay_epochs = 0\n",
    "mvc_decoder_style = \"inner product\"\n",
    "ecs_threshold = config.ecs_thres\n",
    "dab_weight = config.dab_weight\n",
    "\n",
    "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
    "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
    "\n",
    "per_seq_batch_sample = False\n",
    "\n",
    "# settings for optimizer\n",
    "lr = config.lr  # TODO: test learning rate ratio between two tasks\n",
    "lr_ADV = 1e-3  # learning rate for discriminator, used when ADV is True\n",
    "batch_size = config.batch_size\n",
    "eval_batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "schedule_interval = 1\n",
    "\n",
    "# settings for the model\n",
    "fast_transformer = config.fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "embsize = config.layer_size  # embedding dimension\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probability\n",
    "\n",
    "# logging\n",
    "log_interval = 100  # iterations\n",
    "save_eval_interval = config.save_eval_interval  # epochs\n",
    "do_eval_scib_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% validate settings\n",
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "if ADV and DAB:\n",
    "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
    "DAB_separate_optim = True if DAB > 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/clonotype_genes_filtered_counts_data-Jul19-18-55\n"
     ]
    }
   ],
   "source": [
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_dir = Path(\"Tcell_GPT/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bench-user/scGPT/Tcell_GPT/tutorials'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing done on this merged (not integrated dataset):\n",
    "- Cell filtering (doublet, and dead cell filters)\n",
    "- gene filtering (clonotype genes, MT genes removed, overlapping genes between the datasets only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the dataset\n",
    "\n",
    "#Tcell_data_R = sc.read_h5ad(Path.cwd()/\"clono_filtered_counts_adata_for_R.h5ad\")\n",
    "Tcell_data = sc.read_h5ad(Path.cwd()/\"Clono_MT_overlap_gene_filtered_merged_counts.h5ad\") # clonotype and MT- genes removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 25066 × 15093\n",
       "    obs: 'cell_id', 'nCount_RNA', 'nFeature_RNA', 'patient_id', 'percent_mito', 'Response_3m', 'sample_source', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'n_genes'\n",
       "    var: 'gene_name', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'\n",
       "    uns: 'hvg'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tcell_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing (cell filtering)\n",
    "- remove dead cells (percent mito filtering + min_gene filtering)\n",
    "- remove doublets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NR', 'OR'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tcell_data.obs[\"Response_3m\"] = Tcell_data.obs[\"Response_3m\"].astype(\"category\")\n",
    "Tcell_data.obs[\"patient_id\"] = Tcell_data.obs[\"patient_id\"].astype(\"category\")\n",
    "Tcell_data.obs[\"sample_source\"] = Tcell_data.obs[\"sample_source\"].astype(\"category\")\n",
    "Tcell_data.obs[\"Response_3m\"].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess integrated data for binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conversion to anndata messes up patient id. Switching to feature into category feature did not prevent the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product samples are not available in these patients in our internal dataset. \n",
    "         construct patient_id\n",
    "            Axi-cel      Pt237\n",
    "            Axi-cel      Pt245\n",
    "            Axi-cel      Pt253\n",
    "            Axi-cel      Pt263\n",
    "            Axi-cel      Pt276\n",
    "            Axi-cel      Pt282\n",
    "            Axi-cel      Pt375\n",
    " Bispecific CD19/22      Pt010\n",
    " Bispecific CD19/22      Pt011\n",
    " Bispecific CD19/22      Pt013\n",
    " Bispecific CD19/22      Pt015\n",
    " Bispecific CD19/22      Pt016\n",
    " Bispecific CD19/22      Pt025\n",
    " Bispecific CD19/22      Pt026\n",
    " Bispecific CD19/22      Pt031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# method 1: randomly spliting\n",
    "def ref_query_split(Tcell_data, seed_num, split_ratio):\n",
    "    patient_list = Tcell_data.obs[\"patient_id\"].unique()\n",
    "    np.random.seed(seed_num)\n",
    "    reference_patients = np.random.choice(patient_list, size=int(len(patient_list) * split_ratio), replace=False)\n",
    "    reference_adata = Tcell_data[Tcell_data.obs['patient_id'].isin(reference_patients)]   \n",
    "    query_adata = Tcell_data[~Tcell_data.obs['patient_id'].isin(reference_patients)]  \n",
    "    return reference_adata, query_adata\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_adata, query_adata = ref_query_split(Tcell_data, 1, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ac02', 'ac05', 'ac07', 'ac08', 'ac09', ..., '22', '25', '26', '29', '31']\n",
       "Length: 33\n",
       "Categories (33, object): ['02', '03', '09', '11', ..., 'ac30', 'ac32', 'ac33', 'ac37']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_adata.obs.patient_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_name</th>\n",
       "      <th>highly_variable</th>\n",
       "      <th>highly_variable_rank</th>\n",
       "      <th>means</th>\n",
       "      <th>variances</th>\n",
       "      <th>variances_norm</th>\n",
       "      <th>mt</th>\n",
       "      <th>n_cells_by_counts</th>\n",
       "      <th>mean_counts</th>\n",
       "      <th>pct_dropout_by_counts</th>\n",
       "      <th>total_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FAM87B</td>\n",
       "      <td>True</td>\n",
       "      <td>4106.0</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>1.000080</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>99.996011</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LINC00115</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014841</td>\n",
       "      <td>0.015658</td>\n",
       "      <td>0.859693</td>\n",
       "      <td>False</td>\n",
       "      <td>360</td>\n",
       "      <td>0.014841</td>\n",
       "      <td>98.563792</td>\n",
       "      <td>372.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FAM41C</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.929125</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>99.956116</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SAMD11</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.953518</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>99.976063</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NOC2L</td>\n",
       "      <td>True</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>0.531038</td>\n",
       "      <td>1.174560</td>\n",
       "      <td>1.145456</td>\n",
       "      <td>False</td>\n",
       "      <td>7863</td>\n",
       "      <td>0.531038</td>\n",
       "      <td>68.630815</td>\n",
       "      <td>13311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15350</th>\n",
       "      <td>FAM224B</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.901687</td>\n",
       "      <td>False</td>\n",
       "      <td>42</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>99.832442</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15351</th>\n",
       "      <td>TTTY14</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>0.005166</td>\n",
       "      <td>0.944615</td>\n",
       "      <td>False</td>\n",
       "      <td>107</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>99.573127</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15352</th>\n",
       "      <td>KDM5D</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.076358</td>\n",
       "      <td>0.090718</td>\n",
       "      <td>0.967450</td>\n",
       "      <td>False</td>\n",
       "      <td>1698</td>\n",
       "      <td>0.076358</td>\n",
       "      <td>93.225884</td>\n",
       "      <td>1914.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15353</th>\n",
       "      <td>TTTY10</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>0.006149</td>\n",
       "      <td>0.855248</td>\n",
       "      <td>False</td>\n",
       "      <td>146</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>99.417538</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15354</th>\n",
       "      <td>EIF1AY</td>\n",
       "      <td>True</td>\n",
       "      <td>740.0</td>\n",
       "      <td>0.778265</td>\n",
       "      <td>2.793601</td>\n",
       "      <td>1.531370</td>\n",
       "      <td>False</td>\n",
       "      <td>7936</td>\n",
       "      <td>0.778265</td>\n",
       "      <td>68.339583</td>\n",
       "      <td>19508.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15093 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gene_name  highly_variable  highly_variable_rank     means  variances  \\\n",
       "1         FAM87B             True                4106.0  0.000040   0.000040   \n",
       "2      LINC00115            False                   NaN  0.014841   0.015658   \n",
       "3         FAM41C            False                   NaN  0.000439   0.000439   \n",
       "4         SAMD11            False                   NaN  0.000239   0.000239   \n",
       "5          NOC2L             True                2040.0  0.531038   1.174560   \n",
       "...          ...              ...                   ...       ...        ...   \n",
       "15350    FAM224B            False                   NaN  0.001715   0.001792   \n",
       "15351     TTTY14            False                   NaN  0.004548   0.005166   \n",
       "15352      KDM5D            False                   NaN  0.076358   0.090718   \n",
       "15353     TTTY10            False                   NaN  0.005944   0.006149   \n",
       "15354     EIF1AY             True                 740.0  0.778265   2.793601   \n",
       "\n",
       "       variances_norm     mt  n_cells_by_counts  mean_counts  \\\n",
       "1            1.000080  False                  1     0.000040   \n",
       "2            0.859693  False                360     0.014841   \n",
       "3            0.929125  False                 11     0.000439   \n",
       "4            0.953518  False                  6     0.000239   \n",
       "5            1.145456  False               7863     0.531038   \n",
       "...               ...    ...                ...          ...   \n",
       "15350        0.901687  False                 42     0.001715   \n",
       "15351        0.944615  False                107     0.004548   \n",
       "15352        0.967450  False               1698     0.076358   \n",
       "15353        0.855248  False                146     0.005944   \n",
       "15354        1.531370  False               7936     0.778265   \n",
       "\n",
       "       pct_dropout_by_counts  total_counts  \n",
       "1                  99.996011           1.0  \n",
       "2                  98.563792         372.0  \n",
       "3                  99.956116          11.0  \n",
       "4                  99.976063           6.0  \n",
       "5                  68.630815       13311.0  \n",
       "...                      ...           ...  \n",
       "15350              99.832442          43.0  \n",
       "15351              99.573127         114.0  \n",
       "15352              93.225884        1914.0  \n",
       "15353              99.417538         149.0  \n",
       "15354              68.339583       19508.0  \n",
       "\n",
       "[15093 rows x 11 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_adata.var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 = OR; 0 = NR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and pre-process data\n",
    "We follow the standard scGPT data pre-processing pipelines for the cell-type annotation task. Note that since now we have two datasets at hand (i.e., reference and query data), the same pre-prepocessing steps need to be applied to both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"clonotype_genes_filtered_counts_data\":\n",
    "    # data_dir = Path(\"./sample_data_cell_ann\")\n",
    "    adata = ref_adata\n",
    "    adata_test = query_adata\n",
    "    adata.obs[\"Response_3m\"] = adata.obs[\"Response_3m\"].astype(\"category\")\n",
    "    adata_test.obs[\"Response_3m\"] = adata_test.obs[\"Response_3m\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"str_batch\"] = \"1\"          \n",
    "    adata.var.set_index(\"gene_name\", inplace=True) # using gene name directly as index\n",
    "    # fixed typo here adata_test.var instead of adata.var\n",
    "    adata_test.var.set_index(\"gene_name\", inplace=True)\n",
    "    data_is_raw = True\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\") # concatenate reference and query data\n",
    "                \n",
    "# make the batch category column\n",
    "batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "response_id_labels = adata.obs[\"Response_3m\"].astype(\"category\").cat.codes.values #convert categories into integers\n",
    "responses = adata.obs[\"Response_3m\"].unique()\n",
    "num_types = len(np.unique(response_id_labels))\n",
    "id2type = dict(enumerate(adata.obs[\"Response_3m\"].astype(\"category\").cat.categories))\n",
    "adata.obs[\"response_id\"] = response_id_labels\n",
    "adata.var[\"gene_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response_id\n",
       "1    13160\n",
       "0    11906\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs.response_id.value_counts()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 14986/15093 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from ../model/best_model.pt, the model args will override the config ../model/args.json.\n"
     ]
    }
   ],
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 3051.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tcell_data.X.min(), Tcell_data.X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_adata.X.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not sure why this integrated dataset is taking substantially longer to run??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to change the processor so it doesn't raise errors\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\", # \"counts\" # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total= 1e4,  # 3. whether to normalize the raw data and to what sum ## no need to normalize since input data has been normalized\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "adata_test = adata[adata.obs[\"str_batch\"] == \"1\"]\n",
    "adata = adata[adata.obs[\"str_batch\"] == \"0\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning took 21 minutes to run for the scanorama dataset, because the numbers are continuous and it takes more time to divide the samples into buckets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got an error during binning so I restarted the kernal and error fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Log1p transforming ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Log1p transforming ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "preprocessor(adata, batch_key=None)\n",
    "preprocessor(adata_test, batch_key=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref and query data merged into a single adata and preprocessed together and divided into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the code block below: only batch id, response labels and count matrix is fed into the model.\n",
    "\n",
    "modification made: validation set from 0.1 to 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize input data for model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "responses_labels = adata.obs[\"response_id\"].tolist()  # make sure count from 0\n",
    "responses_labels = np.array(responses_labels)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_response_labels, \n",
    "    valid_response_labels,\n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts, responses_labels, batch_ids, test_size=0.2, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 12304, \n",
      "\t feature length: 3001\n",
      "scGPT - INFO - valid set number of samples: 3076, \n",
      "\t feature length: 3001\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 3001 genes are used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    print(\n",
    "        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    tensor_response_labels_train = torch.from_numpy(train_response_labels).long()\n",
    "    tensor_response_labels_valid = torch.from_numpy(valid_response_labels).long()\n",
    "\n",
    "    if sort_seq_batch:  # TODO: update to random pick seq source in each traning batch\n",
    "        train_sort_ids = np.argsort(train_batch_labels)\n",
    "        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        input_values_train = input_values_train[train_sort_ids]\n",
    "        target_values_train = target_values_train[train_sort_ids]\n",
    "        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "        tensor_response_labels_train = tensor_response_labels_train[train_sort_ids]\n",
    "\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "        tensor_response_labels_valid = tensor_response_labels_valid[valid_sort_ids]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "        \"batch_labels\": tensor_batch_labels_train,\n",
    "        \"response_labels\": tensor_response_labels_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "        \"response_labels\": tensor_response_labels_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load the pre-trained scGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('self', 'ntoken', 'd_model', 'nhead', 'd_hid', 'nlayers', 'nlayers_cls', 'n_cls', 'vocab', 'dropout', 'pad_token', 'pad_value', 'do_mvc', 'do_dab', 'use_batch_labels', 'num_batch_labels', 'domain_spec_batchnorm', 'input_emb_style', 'n_input_bins', 'cell_emb_style', 'mvc_decoder_style', 'ecs_threshold', 'explicit_zero_prob', 'use_fast_transformer', 'fast_transformer_backend', 'pre_norm', 'transfer_learning', 'transfer_hidden_dim', 'transfer_dropout', 'use_affine', 'encoder_layers')\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import scgpt.model\n",
    "importlib.reload(scgpt.model)\n",
    "\n",
    "# Verify that the changes have been loaded\n",
    "print(scgpt.model.TransformerModel.__init__.__code__.co_varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bench-user/scGPT/Tcell_GPT/tutorials/../scgpt/model/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(scg.model.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "scGPT - INFO - Total Pre freeze Params 51505221\n",
      "scGPT - INFO - Total Post freeze Params 51505221\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=num_types if CLS else 1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=MVC,\n",
    "    do_dab=DAB,\n",
    "    use_batch_labels=INPUT_BATCH_LABELS,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=config.DSBN,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    mvc_decoder_style=mvc_decoder_style,\n",
    "    ecs_threshold=ecs_threshold,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    "    ############ new parameters for transfer learning ############\n",
    "    transfer_learning=True,\n",
    "    transfer_hidden_dim=256,\n",
    "    transfer_dropout=0.1,\n",
    "    \n",
    ")\n",
    "\n",
    "############ impletmenting gradual unfreezing of parameter weights ############\n",
    "############ starting lr is different for pre-trained vs non-pretrained weights\n",
    "def get_layer_groups(model):\n",
    "    # Group layers for gradual unfreezing and discriminative fine-tuning\n",
    "    groups = [\n",
    "        {'params': list(model.transfer_classifier.parameters()), 'is_pretrained': False},\n",
    "        {'params': list(model.transformer_encoder.layers[-1].parameters()), 'is_pretrained': True},\n",
    "        {'params': list(model.transformer_encoder.layers[-2].parameters()), 'is_pretrained': True},\n",
    "        {'params': list(model.transformer_encoder.layers[-3].parameters()), 'is_pretrained': True},\n",
    "        {'params': list(model.transformer_encoder.layers[-4].parameters()), 'is_pretrained': True},\n",
    "        {'params': list(model.transformer_encoder.layers[-5].parameters()), 'is_pretrained': True},\n",
    "        {'params': list(model.transformer_encoder.layers[-6].parameters()), 'is_pretrained': True},\n",
    "        {'params': list(model.transformer_encoder.layers[-7].parameters()), 'is_pretrained': True},\n",
    "        {'params': list(model.transformer_encoder.layers[-8].parameters()), 'is_pretrained': True},\n",
    "        {'params': list(model.encoder.parameters()) + list(model.value_encoder.parameters()), 'is_pretrained': True},\n",
    "    ]\n",
    "    return groups\n",
    "\n",
    "def set_lr_for_groups(groups, base_lr, pretrained_lr_factor=0.1, lr_decay=0.9):\n",
    "    # Set learning rates for different groups\n",
    "    param_groups = []\n",
    "    for i, group in enumerate(groups):\n",
    "        if group['is_pretrained']:\n",
    "            lr = base_lr * pretrained_lr_factor * (lr_decay ** i)\n",
    "        else:\n",
    "            lr = base_lr * (lr_decay ** i)\n",
    "        \n",
    "        param_groups.append({\n",
    "            'params': group['params'],\n",
    "            'lr': lr,\n",
    "            'is_pretrained': group['is_pretrained']\n",
    "        })\n",
    "    return param_groups\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "\n",
    "\n",
    "# # Freeze all pre-decoder weights\n",
    "# for name, para in model.named_parameters():\n",
    "#     print(\"-\"*20)\n",
    "#     print(f\"name: {name}\")\n",
    "#     if config.freeze and \"encoder\" in name and \"transformer_encoder\" not in name:\n",
    "#     # if config.freeze and \"encoder\" in name:\n",
    "#         print(f\"freezing weights for: {name}\")\n",
    "#         para.requires_grad = False\n",
    "\n",
    "post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "logger.info(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "logger.info(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "wandb.log(\n",
    "        {\n",
    "            \"info/pre_freeze_param_count\": pre_freeze_param_count,\n",
    "            \"info/post_freeze_param_count\": post_freeze_param_count,\n",
    "        },\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "wandb.watch(model)\n",
    "\n",
    "if ADV:\n",
    "    discriminator = AdversarialDiscriminator(\n",
    "        d_model=embsize,\n",
    "        n_cls=num_batch_types,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosen whether to finetune the whole model or just the added layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_original_model(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"transfer_classifier\" not in name:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "def unfreeze_all_parameters(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Apply the function to your model\n",
    "#freeze_original_model(model)\n",
    "unfreeze_all_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify what weights are being trained and what are frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: transfer_classifier.0.weight\n",
      "Trainable: transfer_classifier.0.bias\n",
      "Trainable: transfer_classifier.3.weight\n",
      "Trainable: transfer_classifier.3.bias\n",
      "Trainable: transfer_classifier.6.weight\n",
      "Trainable: transfer_classifier.6.bias\n",
      "Trainable: transfer_classifier.9.weight\n",
      "Trainable: transfer_classifier.9.bias\n",
      "Trainable: encoder.embedding.weight\n",
      "Trainable: encoder.enc_norm.weight\n",
      "Trainable: encoder.enc_norm.bias\n",
      "Trainable: value_encoder.linear1.weight\n",
      "Trainable: value_encoder.linear1.bias\n",
      "Trainable: value_encoder.linear2.weight\n",
      "Trainable: value_encoder.linear2.bias\n",
      "Trainable: value_encoder.norm.weight\n",
      "Trainable: value_encoder.norm.bias\n",
      "Trainable: transformer_encoder.layers.0.self_attn.in_proj_weight\n",
      "Trainable: transformer_encoder.layers.0.self_attn.in_proj_bias\n",
      "Trainable: transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "Trainable: transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "Trainable: transformer_encoder.layers.0.linear1.weight\n",
      "Trainable: transformer_encoder.layers.0.linear1.bias\n",
      "Trainable: transformer_encoder.layers.0.linear2.weight\n",
      "Trainable: transformer_encoder.layers.0.linear2.bias\n",
      "Trainable: transformer_encoder.layers.0.norm1.weight\n",
      "Trainable: transformer_encoder.layers.0.norm1.bias\n",
      "Trainable: transformer_encoder.layers.0.norm2.weight\n",
      "Trainable: transformer_encoder.layers.0.norm2.bias\n",
      "Trainable: transformer_encoder.layers.1.self_attn.in_proj_weight\n",
      "Trainable: transformer_encoder.layers.1.self_attn.in_proj_bias\n",
      "Trainable: transformer_encoder.layers.1.self_attn.out_proj.weight\n",
      "Trainable: transformer_encoder.layers.1.self_attn.out_proj.bias\n",
      "Trainable: transformer_encoder.layers.1.linear1.weight\n",
      "Trainable: transformer_encoder.layers.1.linear1.bias\n",
      "Trainable: transformer_encoder.layers.1.linear2.weight\n",
      "Trainable: transformer_encoder.layers.1.linear2.bias\n",
      "Trainable: transformer_encoder.layers.1.norm1.weight\n",
      "Trainable: transformer_encoder.layers.1.norm1.bias\n",
      "Trainable: transformer_encoder.layers.1.norm2.weight\n",
      "Trainable: transformer_encoder.layers.1.norm2.bias\n",
      "Trainable: transformer_encoder.layers.2.self_attn.in_proj_weight\n",
      "Trainable: transformer_encoder.layers.2.self_attn.in_proj_bias\n",
      "Trainable: transformer_encoder.layers.2.self_attn.out_proj.weight\n",
      "Trainable: transformer_encoder.layers.2.self_attn.out_proj.bias\n",
      "Trainable: transformer_encoder.layers.2.linear1.weight\n",
      "Trainable: transformer_encoder.layers.2.linear1.bias\n",
      "Trainable: transformer_encoder.layers.2.linear2.weight\n",
      "Trainable: transformer_encoder.layers.2.linear2.bias\n",
      "Trainable: transformer_encoder.layers.2.norm1.weight\n",
      "Trainable: transformer_encoder.layers.2.norm1.bias\n",
      "Trainable: transformer_encoder.layers.2.norm2.weight\n",
      "Trainable: transformer_encoder.layers.2.norm2.bias\n",
      "Trainable: transformer_encoder.layers.3.self_attn.in_proj_weight\n",
      "Trainable: transformer_encoder.layers.3.self_attn.in_proj_bias\n",
      "Trainable: transformer_encoder.layers.3.self_attn.out_proj.weight\n",
      "Trainable: transformer_encoder.layers.3.self_attn.out_proj.bias\n",
      "Trainable: transformer_encoder.layers.3.linear1.weight\n",
      "Trainable: transformer_encoder.layers.3.linear1.bias\n",
      "Trainable: transformer_encoder.layers.3.linear2.weight\n",
      "Trainable: transformer_encoder.layers.3.linear2.bias\n",
      "Trainable: transformer_encoder.layers.3.norm1.weight\n",
      "Trainable: transformer_encoder.layers.3.norm1.bias\n",
      "Trainable: transformer_encoder.layers.3.norm2.weight\n",
      "Trainable: transformer_encoder.layers.3.norm2.bias\n",
      "Trainable: transformer_encoder.layers.4.self_attn.in_proj_weight\n",
      "Trainable: transformer_encoder.layers.4.self_attn.in_proj_bias\n",
      "Trainable: transformer_encoder.layers.4.self_attn.out_proj.weight\n",
      "Trainable: transformer_encoder.layers.4.self_attn.out_proj.bias\n",
      "Trainable: transformer_encoder.layers.4.linear1.weight\n",
      "Trainable: transformer_encoder.layers.4.linear1.bias\n",
      "Trainable: transformer_encoder.layers.4.linear2.weight\n",
      "Trainable: transformer_encoder.layers.4.linear2.bias\n",
      "Trainable: transformer_encoder.layers.4.norm1.weight\n",
      "Trainable: transformer_encoder.layers.4.norm1.bias\n",
      "Trainable: transformer_encoder.layers.4.norm2.weight\n",
      "Trainable: transformer_encoder.layers.4.norm2.bias\n",
      "Trainable: transformer_encoder.layers.5.self_attn.in_proj_weight\n",
      "Trainable: transformer_encoder.layers.5.self_attn.in_proj_bias\n",
      "Trainable: transformer_encoder.layers.5.self_attn.out_proj.weight\n",
      "Trainable: transformer_encoder.layers.5.self_attn.out_proj.bias\n",
      "Trainable: transformer_encoder.layers.5.linear1.weight\n",
      "Trainable: transformer_encoder.layers.5.linear1.bias\n",
      "Trainable: transformer_encoder.layers.5.linear2.weight\n",
      "Trainable: transformer_encoder.layers.5.linear2.bias\n",
      "Trainable: transformer_encoder.layers.5.norm1.weight\n",
      "Trainable: transformer_encoder.layers.5.norm1.bias\n",
      "Trainable: transformer_encoder.layers.5.norm2.weight\n",
      "Trainable: transformer_encoder.layers.5.norm2.bias\n",
      "Trainable: transformer_encoder.layers.6.self_attn.in_proj_weight\n",
      "Trainable: transformer_encoder.layers.6.self_attn.in_proj_bias\n",
      "Trainable: transformer_encoder.layers.6.self_attn.out_proj.weight\n",
      "Trainable: transformer_encoder.layers.6.self_attn.out_proj.bias\n",
      "Trainable: transformer_encoder.layers.6.linear1.weight\n",
      "Trainable: transformer_encoder.layers.6.linear1.bias\n",
      "Trainable: transformer_encoder.layers.6.linear2.weight\n",
      "Trainable: transformer_encoder.layers.6.linear2.bias\n",
      "Trainable: transformer_encoder.layers.6.norm1.weight\n",
      "Trainable: transformer_encoder.layers.6.norm1.bias\n",
      "Trainable: transformer_encoder.layers.6.norm2.weight\n",
      "Trainable: transformer_encoder.layers.6.norm2.bias\n",
      "Trainable: transformer_encoder.layers.7.self_attn.in_proj_weight\n",
      "Trainable: transformer_encoder.layers.7.self_attn.in_proj_bias\n",
      "Trainable: transformer_encoder.layers.7.self_attn.out_proj.weight\n",
      "Trainable: transformer_encoder.layers.7.self_attn.out_proj.bias\n",
      "Trainable: transformer_encoder.layers.7.linear1.weight\n",
      "Trainable: transformer_encoder.layers.7.linear1.bias\n",
      "Trainable: transformer_encoder.layers.7.linear2.weight\n",
      "Trainable: transformer_encoder.layers.7.linear2.bias\n",
      "Trainable: transformer_encoder.layers.7.norm1.weight\n",
      "Trainable: transformer_encoder.layers.7.norm1.bias\n",
      "Trainable: transformer_encoder.layers.7.norm2.weight\n",
      "Trainable: transformer_encoder.layers.7.norm2.bias\n",
      "Trainable: transformer_encoder.layers.8.self_attn.in_proj_weight\n",
      "Trainable: transformer_encoder.layers.8.self_attn.in_proj_bias\n",
      "Trainable: transformer_encoder.layers.8.self_attn.out_proj.weight\n",
      "Trainable: transformer_encoder.layers.8.self_attn.out_proj.bias\n",
      "Trainable: transformer_encoder.layers.8.linear1.weight\n",
      "Trainable: transformer_encoder.layers.8.linear1.bias\n",
      "Trainable: transformer_encoder.layers.8.linear2.weight\n",
      "Trainable: transformer_encoder.layers.8.linear2.bias\n",
      "Trainable: transformer_encoder.layers.8.norm1.weight\n",
      "Trainable: transformer_encoder.layers.8.norm1.bias\n",
      "Trainable: transformer_encoder.layers.8.norm2.weight\n",
      "Trainable: transformer_encoder.layers.8.norm2.bias\n",
      "Trainable: transformer_encoder.layers.9.self_attn.in_proj_weight\n",
      "Trainable: transformer_encoder.layers.9.self_attn.in_proj_bias\n",
      "Trainable: transformer_encoder.layers.9.self_attn.out_proj.weight\n",
      "Trainable: transformer_encoder.layers.9.self_attn.out_proj.bias\n",
      "Trainable: transformer_encoder.layers.9.linear1.weight\n",
      "Trainable: transformer_encoder.layers.9.linear1.bias\n",
      "Trainable: transformer_encoder.layers.9.linear2.weight\n",
      "Trainable: transformer_encoder.layers.9.linear2.bias\n",
      "Trainable: transformer_encoder.layers.9.norm1.weight\n",
      "Trainable: transformer_encoder.layers.9.norm1.bias\n",
      "Trainable: transformer_encoder.layers.9.norm2.weight\n",
      "Trainable: transformer_encoder.layers.9.norm2.bias\n",
      "Trainable: transformer_encoder.layers.10.self_attn.in_proj_weight\n",
      "Trainable: transformer_encoder.layers.10.self_attn.in_proj_bias\n",
      "Trainable: transformer_encoder.layers.10.self_attn.out_proj.weight\n",
      "Trainable: transformer_encoder.layers.10.self_attn.out_proj.bias\n",
      "Trainable: transformer_encoder.layers.10.linear1.weight\n",
      "Trainable: transformer_encoder.layers.10.linear1.bias\n",
      "Trainable: transformer_encoder.layers.10.linear2.weight\n",
      "Trainable: transformer_encoder.layers.10.linear2.bias\n",
      "Trainable: transformer_encoder.layers.10.norm1.weight\n",
      "Trainable: transformer_encoder.layers.10.norm1.bias\n",
      "Trainable: transformer_encoder.layers.10.norm2.weight\n",
      "Trainable: transformer_encoder.layers.10.norm2.bias\n",
      "Trainable: transformer_encoder.layers.11.self_attn.in_proj_weight\n",
      "Trainable: transformer_encoder.layers.11.self_attn.in_proj_bias\n",
      "Trainable: transformer_encoder.layers.11.self_attn.out_proj.weight\n",
      "Trainable: transformer_encoder.layers.11.self_attn.out_proj.bias\n",
      "Trainable: transformer_encoder.layers.11.linear1.weight\n",
      "Trainable: transformer_encoder.layers.11.linear1.bias\n",
      "Trainable: transformer_encoder.layers.11.linear2.weight\n",
      "Trainable: transformer_encoder.layers.11.linear2.bias\n",
      "Trainable: transformer_encoder.layers.11.norm1.weight\n",
      "Trainable: transformer_encoder.layers.11.norm1.bias\n",
      "Trainable: transformer_encoder.layers.11.norm2.weight\n",
      "Trainable: transformer_encoder.layers.11.norm2.bias\n",
      "Trainable: decoder.fc.0.weight\n",
      "Trainable: decoder.fc.0.bias\n",
      "Trainable: decoder.fc.2.weight\n",
      "Trainable: decoder.fc.2.bias\n",
      "Trainable: decoder.fc.4.weight\n",
      "Trainable: decoder.fc.4.bias\n",
      "Trainable: cls_decoder._decoder.0.weight\n",
      "Trainable: cls_decoder._decoder.0.bias\n",
      "Trainable: cls_decoder._decoder.2.weight\n",
      "Trainable: cls_decoder._decoder.2.bias\n",
      "Trainable: cls_decoder._decoder.3.weight\n",
      "Trainable: cls_decoder._decoder.3.bias\n",
      "Trainable: cls_decoder._decoder.5.weight\n",
      "Trainable: cls_decoder._decoder.5.bias\n",
      "Trainable: cls_decoder.out_layer.weight\n",
      "Trainable: cls_decoder.out_layer.bias\n"
     ]
    }
   ],
   "source": [
    "def verify_freezing(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Trainable: {name}\")\n",
    "        else:\n",
    "            print(f\"Frozen: {name}\")\n",
    "\n",
    "# freeze_original_model(model)\n",
    "verify_freezing(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated the following code transfer learning objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "\n",
    "# new criterion for transfer learning\n",
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "\n",
    "# Separate parameters for transfer learning classifier\n",
    "# transfer_params = [p for n, p in model.named_parameters() if \"transfer_classifier\" in n]\n",
    "# other_params = [p for n,p in model.named_parameters() if \"transfer_classifier\" not in n]\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     [\n",
    "#         {\"params\": other_params, \"lr\": lr*0.1}, # lower lr for pre-trained weights\n",
    "#         {\"params\": transfer_params, \"lr\": lr * 10} # higher lr for transfer learning classifier\n",
    "#     ],\n",
    "#     lr=lr, eps=1e-4 if config.amp else 1e-8\n",
    "\n",
    "# )\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "#     optimizer, schedule_interval, gamma=config.schedule_ratio\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize layer groups\n",
    "groups = get_layer_groups(model)\n",
    "\n",
    "# Set base learning rate and decay\n",
    "base_lr = config.lr  # Use the learning rate from your config\n",
    "lr_decay = 0.9  # You can adjust this value\n",
    "pretrained_lr_factor = 0.1\n",
    "\n",
    "# Initial parameter groups setup\n",
    "param_groups = set_lr_for_groups(groups, base_lr, pretrained_lr_factor)\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, weight_decay=0.01, eps=1e-4 \n",
    "                              if config.amp else 1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "def gradual_unfreeze(model, optimizer, current_epoch, unfreeze_epoch=1, base_lr=1e-3, pretrained_lr_factor=0.1):\n",
    "    groups = get_layer_groups(model)\n",
    "    layers_to_unfreeze = current_epoch // unfreeze_epoch\n",
    "\n",
    "    for i, group in enumerate(groups):\n",
    "        if i < layers_to_unfreeze:\n",
    "            for param in group['params']:\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in group['params']:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # Update learning rates\n",
    "    new_param_groups = set_lr_for_groups(groups[:layers_to_unfreeze], base_lr, pretrained_lr_factor)\n",
    "    for i, (old_group, new_group) in enumerate(zip(optimizer.param_groups, new_param_groups)):\n",
    "        if i < layers_to_unfreeze:\n",
    "            old_group.update(new_group)\n",
    "        else:\n",
    "            old_group['lr'] = 0 \n",
    "        \n",
    "\n",
    "if DAB_separate_optim:\n",
    "    optimizer_dab = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler_dab = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_dab, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "if ADV:\n",
    "    criterion_adv = nn.CrossEntropyLoss()  # consider using label smoothing\n",
    "    optimizer_E = torch.optim.Adam(model.parameters(), lr=lr_ADV)\n",
    "    scheduler_E = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_E, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_ADV)\n",
    "    scheduler_D = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_D, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer, epoch: int) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch. including transfer learning\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    (\n",
    "        total_loss,\n",
    "        total_accuracy,\n",
    "        total_samples,\n",
    "    ) = (0.0, 0.0, 0.0)\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        \n",
    "        batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "        response_labels = batch_data[\"response_labels\"].to(device)\n",
    "        response_labels = response_labels.long() # make sure labels are long type for CrossEntropyLoss\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                transfer_mode = True, # enable transfer mode\n",
    "                CLS=True, # disable original tasks during transfer learning\n",
    "                CCE=False,\n",
    "                MVC=False,\n",
    "                ECS=False,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            # transfer learning loss\n",
    "            transfer_output = output_dict[\"transfer_output\"]\n",
    "            loss = criterion_transfer(transfer_output, response_labels)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Compute accuracy for transfer task\n",
    "            predictions = transfer_output.argmax(dim=1)\n",
    "            accuracy = (predictions == response_labels).float().mean()\n",
    "\n",
    "            # masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            # loss = 0.0\n",
    "            \n",
    "            \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        \n",
    "\n",
    "        batch_size = input_gene_ids.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_accuracy += accuracy.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / total_samples\n",
    "            cur_accuracy = total_accuracy / total_samples\n",
    "            logger.info(f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                        f\"loss {cur_loss:5.2f} | accuracy {cur_accuracy:5.2f}\")\n",
    "\n",
    "        # Log metrics\n",
    "        metrics_to_log = {\n",
    "            \"train/loss\": loss.item(),\n",
    "            \"train/accuracy\": accuracy.item(),\n",
    "        }\n",
    "        wandb.log(metrics_to_log)\n",
    "\n",
    "    return total_loss / num_batches, total_accuracy / num_batches\n",
    "\n",
    "\n",
    "def define_wandb_metrcis():\n",
    "    wandb.define_metric(\"valid/transfer_loss\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/transfer_accuracy\", summary=\"max\", step_metric=\"epoch\")\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "            response_labels = batch_data[\"response_labels\"].to(device)\n",
    "\n",
    "            # Ensure response_labels are long type for CrossEntropyLoss\n",
    "            response_labels = response_labels.long()\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                    transfer_mode = True, # enable transfer mode\n",
    "                    CLS=False,  # evaluation does not need CLS or CCE\n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=do_sample_in_train,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "                transfer_output = output_dict[\"transfer_output\"]\n",
    "                loss = criterion_transfer(transfer_output, response_labels)\n",
    "\n",
    "            predictions = transfer_output.argmax(dim=1)\n",
    "            accuracy = (predictions == response_labels).float().mean()\n",
    "\n",
    "\n",
    "            batch_size = input_gene_ids.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_accuracy += accuracy.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_accuracy = total_accuracy / total_samples\n",
    "\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"valid/transfer_loss\": total_loss / total_num,\n",
    "            \"valid/transfer_accuracy\": total_accuracy / total_num,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if return_raw:\n",
    "        return np.concatenate(predictions, axis=0)\n",
    "\n",
    "    return total_loss / total_num, total_accuracy/ total_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Finetune scGPT with task-specific objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "took about an hour to finish training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch   1, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   1 |   100/ 1538 batches | loss  0.69 | accuracy  0.48\n",
      "scGPT - INFO - | epoch   1 |   200/ 1538 batches | loss  0.69 | accuracy  0.48\n",
      "scGPT - INFO - | epoch   1 |   300/ 1538 batches | loss  0.70 | accuracy  0.47\n",
      "scGPT - INFO - | epoch   1 |   400/ 1538 batches | loss  0.69 | accuracy  0.48\n",
      "scGPT - INFO - | epoch   1 |   500/ 1538 batches | loss  0.69 | accuracy  0.48\n",
      "scGPT - INFO - | epoch   1 |   600/ 1538 batches | loss  0.69 | accuracy  0.48\n",
      "scGPT - INFO - | epoch   1 |   700/ 1538 batches | loss  0.69 | accuracy  0.48\n",
      "scGPT - INFO - | epoch   1 |   800/ 1538 batches | loss  0.69 | accuracy  0.49\n",
      "scGPT - INFO - | epoch   1 |   900/ 1538 batches | loss  0.69 | accuracy  0.49\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 42\u001b[0m\n\u001b[1;32m     33\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m prepare_dataloader(\n\u001b[1;32m     34\u001b[0m     valid_data_pt,\n\u001b[1;32m     35\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39meval_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdo_train:\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# pass optimizer to train function\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass current epoch to the train function\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m     49\u001b[0m     model,\n\u001b[1;32m     50\u001b[0m     loader\u001b[38;5;241m=\u001b[39mvalid_loader,\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     52\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m epoch_start_time\n",
      "Cell \u001b[0;32mIn[37], line 58\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     56\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m     57\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     63\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m input_gene_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.apps/conda/envs/scGPT_env/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:416\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    414\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 416\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.apps/conda/envs/scGPT_env/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:314\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    315\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/.apps/conda/envs/scGPT_env/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:314\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    315\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_val_accuracy = 0.0\n",
    "best_model = None\n",
    "define_wandb_metrcis()\n",
    "\n",
    "# Initialize layer groups\n",
    "# groups = get_layer_groups(model)\n",
    "\n",
    "# # Set base learning rate and decay\n",
    "# base_lr = config.lr  # Use the learning rate from your config\n",
    "# lr_decay = 0.9  # You can adjust this value\n",
    "\n",
    "# param_groups = set_lr_for_groups(groups, base_lr, lr_decay)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    ### gradual unfreeze\n",
    "    gradual_unfreeze(model, optimizer, epoch, unfreeze_epoch=1, base_lr=base_lr, pretrained_lr_factor=pretrained_lr_factor)\n",
    "\n",
    "    param_groups = set_lr_for_groups(groups, base_lr, lr_decay)\n",
    "    for group, new_group in zip(optimizer.param_groups, param_groups):\n",
    "        group.update(new_group)\n",
    "\n",
    "    train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(\n",
    "            model,\n",
    "            loader=train_loader,\n",
    "            optimizer=optimizer, # pass optimizer to train function\n",
    "            epoch=epoch, # Pass current epoch to the train function\n",
    "        )\n",
    "    val_loss, val_accuracy = evaluate(\n",
    "        model,\n",
    "        loader=valid_loader,\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss {val_loss:5.4f} | accuracy {val_accuracy:5.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    # Update best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with loss {best_val_loss:5.4f} and accuracy {best_val_accuracy:5.4f}\")\n",
    "\n",
    "    # Optionally, you can also track best model based on accuracy if that's more relevant for your task\n",
    "    # if val_accuracy > best_val_accuracy:\n",
    "    #     best_val_accuracy = val_accuracy\n",
    "    #     best_model = copy.deepcopy(model)\n",
    "    #     best_model_epoch = epoch\n",
    "    #     logger.info(f\"Best model with accuracy {best_val_accuracy:5.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    if DAB_separate_optim:\n",
    "        scheduler_dab.step()\n",
    "    if ADV:\n",
    "        scheduler_D.step()\n",
    "        scheduler_E.step()\n",
    "\n",
    "# After training, log the best model's performance\n",
    "wandb.log({\n",
    "    \"best_val_loss\": best_val_loss,\n",
    "    \"best_val_accuracy\": best_val_accuracy,\n",
    "    \"best_model_epoch\": best_model_epoch\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), 'best_transfer_model.pth')\n",
    "logger.info(f\"Best model saved from epoch {best_model_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% inference\n",
    "def test(model: nn.Module, adata: DataLoader) -> float:\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].A\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    responses_labels = adata.obs[\"response_id\"].tolist()  # make sure count from 0\n",
    "    responses_labels = np.array(responses_labels)\n",
    "\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    tokenized_test = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=include_zero_gene,\n",
    "    )\n",
    "\n",
    "    input_values_test = random_mask_value(\n",
    "        tokenized_test[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "\n",
    "    test_data_pt = {\n",
    "        \"gene_ids\": tokenized_test[\"genes\"],\n",
    "        \"values\": input_values_test,\n",
    "        \"target_values\": tokenized_test[\"values\"],\n",
    "        \"batch_labels\": torch.from_numpy(batch_ids).long(),\n",
    "        \"response_labels\": torch.from_numpy(responses_labels).long(),\n",
    "    }\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=SeqDataset(test_data_pt),\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=min(len(os.sched_getaffinity(0)), eval_batch_size // 2),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    predictions = evaluate(\n",
    "        model,\n",
    "        loader=test_loader,\n",
    "        return_raw=True,\n",
    "    )\n",
    "\n",
    "    # compute accuracy, precision, recall, f1\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    accuracy = accuracy_score(responses_labels, predictions)\n",
    "    precision = precision_score(responses_labels, predictions, average=\"macro\")\n",
    "    recall = recall_score(responses_labels, predictions, average=\"macro\")\n",
    "    macro_f1 = f1_score(responses_labels, predictions, average=\"macro\")\n",
    "\n",
    "    logger.info(\n",
    "        f\"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, \"\n",
    "        f\"Macro F1: {macro_f1:.3f}\"\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"test/accuracy\": accuracy,\n",
    "        \"test/precision\": precision,\n",
    "        \"test/recall\": recall,\n",
    "        \"test/macro_f1\": macro_f1,\n",
    "    }\n",
    "\n",
    "    return predictions, responses_labels, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Inference with fine-tuned scGPT model\n",
    "In the cell-type annotation task, the fine-tuned scGPT predicts cell-type labels for query set as inference. The model performance is evaluated on standard classificaton metrics. Here we visualize the predicted labels over the scGPT cell embeddings, and present the confusion matrix for detailed classification performance on the cell-group level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate() got an unexpected keyword argument 'return_raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions, labels, results \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madata_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m adata_test_raw\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [id2type[p] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m predictions]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plot\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 51\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, adata)\u001b[0m\n\u001b[1;32m     41\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     42\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mSeqDataset(test_data_pt),\n\u001b[1;32m     43\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39meval_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 51\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# compute accuracy, precision, recall, f1\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate() got an unexpected keyword argument 'return_raw'"
     ]
    }
   ],
   "source": [
    "predictions, labels, results = test(best_model, adata_test)\n",
    "adata_test_raw.obs[\"predictions\"] = [id2type[p] for p in predictions]\n",
    "\n",
    "# plot\n",
    "palette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] \n",
    "palette_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] + plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"] + plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "palette_ = {c: palette_[i] for i, c in enumerate(responses)}\n",
    "\n",
    "with plt.rc_context({\"figure.figsize\": (6, 4), \"figure.dpi\": (300)}):\n",
    "    sc.pl.umap(\n",
    "        adata_test_raw,\n",
    "        color=[\"Response_3m\", \"predictions\"],\n",
    "        palette=palette_,\n",
    "        show=False,\n",
    "    )\n",
    "    plt.savefig(save_dir / \"results.png\", dpi=300)\n",
    "\n",
    "save_dict = {\n",
    "    \"predictions\": predictions,\n",
    "    \"labels\": labels,\n",
    "    \"results\": results,\n",
    "    \"id_maps\": id2type\n",
    "}\n",
    "with open(save_dir / \"results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(save_dict, f)\n",
    "\n",
    "results[\"test/cell_umap\"] = wandb.Image(\n",
    "    str(save_dir / \"results.png\"),\n",
    "    caption=f\"predictions macro f1 {results['test/macro_f1']:.3f}\",\n",
    ")\n",
    "wandb.log(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m      2\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(responses)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m([id2type[p] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpredictions\u001b[49m]):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m responses:\n\u001b[1;32m      5\u001b[0m         responses\u001b[38;5;241m.\u001b[39mremove(i)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "responses = list(responses)\n",
    "for i in set([id2type[p] for p in predictions]):\n",
    "    if i not in responses:\n",
    "        responses.remove(i)\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm = pd.DataFrame(cm, index=responses[:cm.shape[0]], columns=responses[:cm.shape[1]])\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\".1f\", cmap=\"Blues\")\n",
    "plt.savefig(save_dir / \"confusion_matrix.png\", dpi=300)\n",
    "\n",
    "results[\"test/confusion_matrix\"] = wandb.Image(\n",
    "    str(save_dir / \"confusion_matrix.png\"),\n",
    "    caption=f\"confusion matrix\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model into the save_dir\n",
    "torch.save(best_model.state_dict(), save_dir / \"model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scGPT_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
